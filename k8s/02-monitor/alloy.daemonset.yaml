# alloy-daemonset.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-config
  namespace: default
data:
  config.river: |
    // Scrape Kubernetes pod metrics
    prometheus.scrape "kubernetes_pods" {
      targets = discovery.kubernetes.pods.targets
      forward_to = [prometheus.remote_write.lgtm.receiver]
    }
    
    // Kubernetes pod discovery
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    // Send metrics to your LGTM server
    prometheus.remote_write "lgtm" {
      endpoint {
        url = "http://tabletap-monitor.taila459ef.ts.net:9009/api/v1/push"
        // Add authentication if needed
      }
    }
    
    // Collect logs from pods
    loki.source.kubernetes "pods" {
      targets    = discovery.kubernetes.pods.targets
      forward_to = [loki.write.lgtm.receiver]
    }
    
    // Send logs to Loki
    loki.write "lgtm" {
      endpoint {
        url = "http://tabletap-monitor.taila459ef.ts.net:3100/loki/api/v1/push"
      }
    }

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: alloy
  namespace: default
spec:
  selector:
    matchLabels:
      name: alloy
  template:
    metadata:
      labels:
        name: alloy
    spec:
      containers:
      - name: alloy
        image: grafana/alloy:latest
        args:
        - run
        - /etc/alloy/config.river
        - --storage.path=/var/lib/alloy/data
        - --server.http.listen-addr=0.0.0.0:12345
        volumeMounts:
        - name: config
          mountPath: /etc/alloy
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: alloy-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      serviceAccountName: alloy
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alloy
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alloy
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alloy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy
subjects:
- kind: ServiceAccount
  name: alloy
  namespace: default
```

### Step 2: Configure Grafana Dashboards

Once metrics are flowing, import these dashboards in Grafana:
```
Kubernetes Dashboards (Import by ID in Grafana):
├── 15757: Kubernetes Cluster Monitoring
├── 15758: Kubernetes / Views / Pods
├── 15759: Kubernetes / Compute Resources / Pod
└── 8588: Kubernetes Deployment Statefulset Daemonset metrics